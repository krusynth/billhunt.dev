<!DOCTYPE html>
<html lang="en">
<!--.  ,--,                          ,-. ,-.
 |  | /  /                         __| |_| |
 |  |/  / _ __ _   _ ____ _   _ _ (__, ._| |___
 |      \| `__| | | Y ___| |_| | `_  \ | | ,_. \
 |  |\   \ |  | |_| |___ \___, | | | | | | | | |
 |__| \___\|  '.__,_|____/ __| |_| |_|_| |_| |_|
                          '----><head>
 <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LLMs Are Not Ready for Government Use | Bill Hunt</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="LLMs Are Not Ready for Government Use" />
<meta name="author" content="Bill Hunt" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The latest buzzword in govtech these days is Large Language Models (LLMs). However, they are too risky for the government to trust." />
<meta property="og:description" content="The latest buzzword in govtech these days is Large Language Models (LLMs). However, they are too risky for the government to trust." />
<link rel="canonical" href="https://billhunt.dev/blog/2023/10/09/llms-are-not-government-ready/" />
<meta property="og:url" content="https://billhunt.dev/blog/2023/10/09/llms-are-not-government-ready/" />
<meta property="og:site_name" content="Bill Hunt" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-09T17:23:48+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LLMs Are Not Ready for Government Use" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Bill Hunt"},"dateModified":"2023-10-09T17:23:48+00:00","datePublished":"2023-10-09T17:23:48+00:00","description":"The latest buzzword in govtech these days is Large Language Models (LLMs). However, they are too risky for the government to trust.","headline":"LLMs Are Not Ready for Government Use","mainEntityOfPage":{"@type":"WebPage","@id":"https://billhunt.dev/blog/2023/10/09/llms-are-not-government-ready/"},"url":"https://billhunt.dev/blog/2023/10/09/llms-are-not-government-ready/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/main.css?1717012121">
  <link rel="stylesheet" media="print" href="/assets/css/print.css?1717012121">

  <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />
  <link rel="alternate" type="application/atom+xml" title=" - Posts" href="https://billhunt.dev/feed.xml" />
  <link rel="alternate" type="application/atom+xml" title=" - Featured Jobs " href="https://billhunt.dev/jobs.xml" />
  <link rel="alternate" type="application/atom+xml" title=" - USAJobs List" href="https://billhunt.dev/jobs.xml" />
  <link rel="alternate" type="application/atom+xml" title="Enderprise Architecture YouTube Series" href="https://www.youtube.com/feeds/videos.xml?channel_id=UCSL7BIdwgBEZ09BpD9xPPYQ">
  <link rel="meta" type="application/rdf+xml" title="FOAF" href="https://billhunt.dev/foaf.rdf" />
  

  <link rel="me" href="https://mastodon.publicinterest.town/@krusynth" />
  <link rel="me" href="https://mastodon.cloud/@krusynth" />

  <script src="https://static.billhunt.dev/js/jquery.min.js?1717012121"></script>
  <script src="https://static.billhunt.dev/assets/js/unpoly.min.js?1717012121"></script>
  <script src="https://static.billhunt.dev/assets/js/fontawesome-6/fontawesome.min.js?1717012121"></script>
  <script type="module" src="/assets/js/main.js?1717012121"></script>
  <script src="/assets/js/search.js?1717012121"></script>
  <script src="/assets/js/jobs.js?1717012121"></script>
  <script src="https://static.billhunt.dev/assets/js/blink-polyfill.js"></script>
</head>
<body><nav class="navbar navbar-expand-lg container">
  <div class="branding">
    <a class="navbar-brand" rel="author" href="/" title="Bill Hunt | Home" up-follow>Bill Hunt</a>
    <div class="about-description">
      U.S. Gov Civic Technologist & Policy Expert
    </div>
  </div>

  <div class="navlink-container">
    <ul class="nav navbar-nav nav-pages">
      <li class="nav-item nav-page nav-search" id="nav-search">
        <a class="nav-link" href="/search/" title="Search" up-follow><span class="fa-magnifying-glass fas"></span></a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/blog/" up-follow>Blog</a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/about/" up-follow>About</a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/shop/" up-follow>Shop</a>
      </li>
       <li class="nav-item nav-page">
        <a class="nav-link" href="/jobs/" up-follow>Jobs</a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/links/" up-follow>Links</a>
      </li>
      <li class="mastodon nav-item nav-social">
        <a href="https://mastodon.publicinterest.town/@krusynth" title="Mastodon" class="nav-link" rel="me"><span class="fab fa-mastodon icon"></span></a>
      </li>
        
      <li class="github nav-item nav-social">
        <a href="https://github.com/krusynth" title="GitHub" class="nav-link" rel="me"><span class="fab fa-github icon"></span></a>
      </li>
        
      <li class="linkedin nav-item nav-social">
        <a href="https://www.linkedin.com/in/krusynth/" title="LinkedIn" class="nav-link" rel="me"><span class="fab fa-linkedin icon"></span></a>
      </li>
        
      
    </ul>
  </div>

  <div class="audioplayer-block">
    <div id="audioplayer" class="audioplayer">
      <button id="playpause" class="player-button"><span class="fas fa-play" id="playbutton"></span><span class="fas fa-pause hide" id="pausebutton"></span></button>
      <select id="audiofile" class="player-button tracklist" aria-label="Track to Play">
        <option value="Nine_Inch_Nails-Head_Like_A_Hole.midi.mp3">Nine Inch Nails - Head Like A Hole</option>
        <option value="Prodigy-Breathe.midi.mp3">Prodigy - Breathe</option>
        <option value="Sisters_of_Mercy-Temple_of_Love.midi.mp3">Sisters of Mercy - Temple of Love</option>
        <option value="The_Cult-She_Sells_Sancturary.midi.mp3">The Cult - She Sells Sancturary</option>
        <option value="KMFDM-Megalomaniac.midi.mp3">KMFDM - Megalomaniac</option>
        <option value="Tool-Aenima.midi.mp3">Tool - Aenima</option>
        <option value="Rammstein-Engel.midi.mp3">Rammstein - Engel</option>
        <option value="Rancid-TimeBomb.midi.mp3">Rancid - TimeBomb</option>
        <option value="Mighty_Mighty_Bosstones-The_Impression_That_I_Get.midi.mp3">Mighty Mighty Bosstones - The Impression That I Get</option>
        <option value="Offspring-All_I_Want.midi.mp3">Offspring - All I Want</option>
        <option value="Smashing_Pumpkins-1979.midi.mp3">Smashing Pumpkins - 1979</option>
        <option value="Nirvana-Heart_Shaped_Box.midi.mp3">Nirvana - Heart Shaped Box</option>
      </select>
    </div>
  </div>

  <section class="about-header">
    <div class="navbar-tagline" id="tagline">Move carefully and fix things</div>
  </section>
</nav>
<div class="content" id="main" up-main>
        <section class="banner stickers">
    If you’d like to show everyone that you also want to <strong>Move Carefully and Fix Things</strong> or that you're a <strong>Govpunk</strong>, I’d like to send you a sticker for free! <a href="/stickers/"><strong>Here’s how to get yours!</strong></a>
</section>

<article class="post-single page-llms-are-not-ready-for-government-use" data-pagefind-body>
  <header class="post-header">
    <h1 class="post-title">LLMs Are Not Ready for Government Use</h1>
    <time datetime="2023-10-09T17:23:48+00:00" class="post-date">
      2023.10.09
    </time>
  </header>
  <section class="post-content">
      
      
      
      
      

    <p>The latest buzzword in govtech these days is Artificial Intelligence (AI), specifically that buzzy new flavor, <strong>Large Language Models</strong> or LLMs.  Any day now, we expect the White House to <a href="https://federalnewsnetwork.com/artificial-intelligence/2023/09/draft-omb-memo-details-10-new-requirements-to-manage-ai/">release updated guidance on AI usage</a> in government. Given the <a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/">milquetoast previous offering</a> that was released earlier this year, it is unlikely that we will see an aggressive stance on limiting usage.</p>

<p>Government technology is entirely about <em>risk avoidance</em>, though we call it “risk mitigation” or “risk management.” The current state of LLMs expose the government to risks that cannot be managed or avoided. There are <strong>four primary concerns</strong> where any CIO should be evaluating if these tools will put their organization well outside of an acceptable risk tolerance.</p>

<h2 id="llms-in-a-nutshell">LLMs in a Nutshell</h2>

<p>Before we dive in, it’s important to understand how LLMs work.  The secret sauce is right there in the name - these are artificial intelligence models trained on large amounts of text to process language.</p>

<p>To massively oversimplify, they make predictions based on how often they see certain words together. It’s like those word-association games: if I say “sail,” you’ll probably say “boat.” And if you’re of a similar age and background as myself, you might think of <a href="https://www.youtube.com/watch?v=tgIqecROs5M">a certain song</a>. And moreover, if you’re a person who looks at cat memes on the internet, you may think of a <a href="https://www.youtube.com/watch?v=Awf45u6zrP0">certain video</a>.</p>

<p>LLMs look at a vast amount of content - training data sets - fed to them by their creators, to create the base knowledge to process information.  On top of that, they’re sometimes fed additional data by a given customer, to tailor their abilities to predict even further for a specific use case. These two sources of data are combined to make predictions when you ask them questions.</p>

<h2 id="risk-1-exposure-of-data">Risk 1: Exposure of Data</h2>

<p>Table stakes for any government software system of any sort is that it cannot leak private data. If you can’t assure that, you have no business being in the field. Though, even some of the <a href="https://techcrunch.com/2023/09/08/microsoft-hacker-china-government-storm-0558/">biggest players are struggling with these basics</a> these days.</p>

<p>At a minimum, LLMs should expose neither the additional training data that a customer feeds into the system, nor any information that is leaked by asking questions of the data. Effectively, a customer’s interactions with the system should be siloed away from all other data.</p>

<p>Although the government can attempt to prohibit federal staff from putting sensitive information into queries of the systems, inevitably someone will break the rule, as was seen in the <a href="https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/">Samsung leak in ChatGPT</a>. But as these platforms are still very new and being rapidly - perhaps <em>carelessly</em> - developed, even experienced developers working for megacorporations can accidentally expose huge amounts of sensitive information with a single accidental command, <a href="https://techcrunch.com/2023/09/18/microsoft-ai-researchers-accidentally-exposed-terabytes-of-internal-sensitive-data/">as happened with Microsoft earlier this year</a>.</p>

<p>That’s the most obvious concern, and it exists for any IT system. The other concerns require a hard look at how the LLMs actually work.</p>

<h2 id="risk-2-dangerous-source-data">Risk 2: Dangerous Source Data</h2>

<p>LLMs must be trained on <em>something</em>, various large bodies of text. However, companies are not disclosing where they are getting this training data from, which presents a series of issues for the government.</p>

<p>Most notably, there is <a href="https://www.theatlantic.com/technology/archive/2023/09/books3-ai-training-meta-copyright-infringement-lawsuit/675411/">extensive documented evidence</a> that most companies are using copyrighted works to feed their training data. Many are being blatant about this theft of intellectual property, requiring that <a href="https://www.theverge.com/2023/9/28/23894779/google-ai-extended-training-data-toggle-bard-vertex">content creators <em>opt out</em> of being included in training data</a>.</p>

<p>(I’ll also mention that I have personally attempted to opt out from such training activities, and in some cases been completely unable to do so. For instance, after filling out <a href="https://www.facebook.com/help/contact/1266025207620918">Facebook’s opt out form</a> I was contacted by their support team who effectively said they don’t have a way to do that.)</p>

<p>Imagine if you’re a writer, and you had to go tell every single other writer individually that they’re not allowed to copy your text and use it in their own work - that would be impossible! These companies know what they’re doing is in violation of copyright, and are avoiding prosecution by not giving out their list of sources. Although the obvious illegality of this has not yet been tested in court, <a href="https://www.theartnewspaper.com/2023/09/02/artificial-intelligence-lawsuit-decision-us-copyright-law">the Supreme Court <em>has</em> ruled that the resulting content generated by these models is not protected</a>.</p>

<p>Any use of these technologies where the result is a public document may result in huge liability for an organization. For the government, since everything is FOIA-able, any such document may present an issue. This is a risk the government cannot afford to take.</p>

<h2 id="risk-3-offensive-source-data">Risk 3: Offensive Source Data</h2>

<p>The last thing that must be considered is the nature of the initial training data. It’s clear that most companies are training their data off of content on the internet, which may or may not be heavily biased. We’ve seen the largest corporations in the world attempt to create chatbots repeatedly, only to have them be filled with racist language and questionable content. (I’ve discussed this previously in my <a href="https://billhunt.dev/blog/2020/12/21/ai-ml-rpa-principles/">Principles for Automation</a> article.)</p>

<p>Microsoft has failed at this <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">time</a> and <a href="https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams">time again</a>. Most recently, their AI-powered search became filled with <a href="https://www.cpomagazine.com/cyber-security/microsoft-bing-ai-chatbot-poisoned-with-malicious-ads/">malicious advertisements</a>. This bias has been exposed in a number of other ways, such as <a href="https://www.npr.org/sections/goatsandsoda/2023/10/06/1201840678/ai-was-asked-to-create-images-of-black-african-docs-treating-white-kids-howd-it-">Midjourney creating offensive images of Black doctors</a>.  These toys are reading horrible things on the internet and spitting them back out.</p>

<p>No organization wants to field a lawsuit due to a racist chatbot dealing with the public or an employee.</p>

<h2 id="risk-4-misinformation">Risk 4: Misinformation</h2>

<p>Even if these LLMs are set to filter out this sort of content, the simple fact remains that they are unable to produce truthful information. <a href="https://www.washingtonpost.com/technology/2023/10/07/amazon-alexa-news-2020-election-misinformation/">Amazon’s Alexa recently started spreading 2020 election misinformation</a>. A study of <a href="https://www.theregister.com/2023/08/07/chatgpt_stack_overflow_ai/">ChatGPT showed less than half of its answers as correct</a>.</p>

<p>For a government agency, this could prove disastrous. If a group of malicious actors were to feed deliberately-incorrect information into a known training data set, they could manipulate the outcomes of that LLM - effectively a modern exploit similar to a “<a href="Google Bomb">Google Bomb</a>.” For instance, if an agency was looking for a particular type of fraud, and someone poisoned the source data with “Company X commits fraud all the time,” the bots will find ways to inject that into the results, potentially setting up that company for unwarranted investigation. Since social media is a known source of training data for many companies, this is likely <em>occurring already</em>.</p>

<p>Moreover, the risk of foreign actors exploiting this is huge. <a href="https://www.wired.com/story/russia-secondary-infektion-disinformation/">Russia’s manipulation of the media to run disinformation campaigns</a> is well known at this point, but the depth of this infection is unknown. This poisoning could have a disastrous effect on law enforcement or military organizations using these sorts of tools to gain an edge. A methodology could also exploit the opposite. For instance, training false “all clear” signals to cybersecurity bots being used to detect network or system exploits, to better hide cyber attacks. The possibilities are endless.</p>

<p>If these bots cannot even be trusted to produce true information, why would anyone consider using them? Why would we cheat and copy the answers off the person who always fails the test? Especially if that person wants us to fail?</p>

<h2 id="inevitability">Inevitability</h2>

<p>I remain a futurist, and a cyberpunk, yet I find myself frequently <a href="https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai">reconsidering the Luddites</a>.  The increased use of AI is inevitable, and I do have faith that the models will be improved over time.</p>

<p>But for now, the current state of the art is a bunch of lying, racist, garbage bots - snake oil salesmen and shysters of the modern age. The government should not put its trust in these tools for a long, long time. They are simply outside of our tolerance for risk.</p>

  </section>

</article>

    </div>
<footer class="page-footer">
  <div class="webring">
    <script src="https://pitwebring.billhunt.dev/webring.js"></script>
    <script>showWebring(true);</script>
  </div>

  <div class="webring">
    <div class="webring-title">
      Member of the <a href="https://webring.obeythesystem.com/" rel="nofollow">Obey the System Webring</a>.
    </div>
    <nav class="webring-nav">
      <ul class="webring-links">
        <li><a href="https://webring.obeythesystem.com/page?=previous" rel="nofollow">← previous</a></li>
        <li><a href="https://webring.obeythesystem.com/page?=next" rel="nofollow">next →</a></li>
      </ul>
    </nav>
  </div>

  <div class="oldschool-buttons">
    <a href="https://pitwebring.billhunt.dev/" class="old-button" title="Public Interest Tech Webring"><img src="/assets/images/buttons/pitwebring-88x31.gif" alt="" aria-hidden="true"></a>
    <a href="https://webring.obeythesystem.com/" class="old-button" rel="nofollow"><img src="https://webring.obeythesystem.com/ots_webring_button_1.gif" alt="OTS Webring Button"></a>
    <span class="old-button"><img src="/assets/images/buttons/human-88x31.gif" alt="Human-Made - No AI"></span>
    <a href="https://jekyllrb.com/" title="Built With Jekyll" class="old-button"><img src="/assets/images/buttons/jekyll-88x31-ani.gif" alt="" aria-hidden="true"></a>
    <a href="https://archive.org/download/netscape-navigator-4.0.4/netscape-navigator-4.0.4.png" class="old-button" title="This links to an image of the Netscape install CD"><img src="/assets/images/buttons/netscape3-88x31-ani.gif" alt="Get Netscape 3.0"></a>
  </div>
</footer>
</body>

</html>
