<!DOCTYPE html>
<html lang="en">
<!--.  ,--,                          ,-. ,-.
 |  | /  /                         __| |_| |
 |  |/  / _ __ _   _ ____ _   _ _ (__, ._| |___
 |      \| `__| | | Y ___| |_| | `_  \ | | ,_. \
 |  |\   \ |  | |_| |___ \___, | | | | | | | | |
 |__| \___\|  '.__,_|____/ __| |_| |_|_| |_| |_|
                          '----><head>
 <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>posts | Bill Hunt</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="posts" />
<meta name="author" content="Bill Hunt" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal website and blog of Bill Hunt" />
<meta property="og:description" content="Personal website and blog of Bill Hunt" />
<link rel="canonical" href="https://billhunt.dev/posts/" />
<meta property="og:url" content="https://billhunt.dev/posts/" />
<meta property="og:site_name" content="Bill Hunt" />
<meta property="og:type" content="website" />
<link rel="next" href="https://billhunt.dev/posts/2/" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="posts" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Bill Hunt"},"description":"Personal website and blog of Bill Hunt","headline":"posts","url":"https://billhunt.dev/posts/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/main.css?1717012122">
  <link rel="stylesheet" media="print" href="/assets/css/print.css?1717012122">

  <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />
  <link rel="alternate" type="application/atom+xml" title=" - Posts" href="https://billhunt.dev/feed.xml" />
  <link rel="alternate" type="application/atom+xml" title=" - Featured Jobs " href="https://billhunt.dev/jobs.xml" />
  <link rel="alternate" type="application/atom+xml" title=" - USAJobs List" href="https://billhunt.dev/jobs.xml" />
  <link rel="alternate" type="application/atom+xml" title="Enderprise Architecture YouTube Series" href="https://www.youtube.com/feeds/videos.xml?channel_id=UCSL7BIdwgBEZ09BpD9xPPYQ">
  <link rel="meta" type="application/rdf+xml" title="FOAF" href="https://billhunt.dev/foaf.rdf" />
  

  <link rel="me" href="https://mastodon.publicinterest.town/@krusynth" />
  <link rel="me" href="https://mastodon.cloud/@krusynth" />

  <script src="https://static.billhunt.dev/js/jquery.min.js?1717012122"></script>
  <script src="https://static.billhunt.dev/assets/js/unpoly.min.js?1717012122"></script>
  <script src="https://static.billhunt.dev/assets/js/fontawesome-6/fontawesome.min.js?1717012122"></script>
  <script type="module" src="/assets/js/main.js?1717012122"></script>
  <script src="/assets/js/search.js?1717012122"></script>
  <script src="/assets/js/jobs.js?1717012122"></script>
  <script src="https://static.billhunt.dev/assets/js/blink-polyfill.js"></script>
</head>
<body><nav class="navbar navbar-expand-lg container">
  <div class="branding">
    <a class="navbar-brand" rel="author" href="/" title="Bill Hunt | Home" up-follow>Bill Hunt</a>
    <div class="about-description">
      U.S. Gov Civic Technologist & Policy Expert
    </div>
  </div>

  <div class="navlink-container">
    <ul class="nav navbar-nav nav-pages">
      <li class="nav-item nav-page nav-search" id="nav-search">
        <a class="nav-link" href="/search/" title="Search" up-follow><span class="fa-magnifying-glass fas"></span></a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/blog/" up-follow>Blog</a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/about/" up-follow>About</a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/shop/" up-follow>Shop</a>
      </li>
       <li class="nav-item nav-page">
        <a class="nav-link" href="/jobs/" up-follow>Jobs</a>
      </li>
      <li class="nav-item nav-page">
        <a class="nav-link" href="/links/" up-follow>Links</a>
      </li>
      <li class="mastodon nav-item nav-social">
        <a href="https://mastodon.publicinterest.town/@krusynth" title="Mastodon" class="nav-link" rel="me"><span class="fab fa-mastodon icon"></span></a>
      </li>
        
      <li class="github nav-item nav-social">
        <a href="https://github.com/krusynth" title="GitHub" class="nav-link" rel="me"><span class="fab fa-github icon"></span></a>
      </li>
        
      <li class="linkedin nav-item nav-social">
        <a href="https://www.linkedin.com/in/krusynth/" title="LinkedIn" class="nav-link" rel="me"><span class="fab fa-linkedin icon"></span></a>
      </li>
        
      
    </ul>
  </div>

  <div class="audioplayer-block">
    <div id="audioplayer" class="audioplayer">
      <button id="playpause" class="player-button"><span class="fas fa-play" id="playbutton"></span><span class="fas fa-pause hide" id="pausebutton"></span></button>
      <select id="audiofile" class="player-button tracklist" aria-label="Track to Play">
        <option value="Nine_Inch_Nails-Head_Like_A_Hole.midi.mp3">Nine Inch Nails - Head Like A Hole</option>
        <option value="Prodigy-Breathe.midi.mp3">Prodigy - Breathe</option>
        <option value="Sisters_of_Mercy-Temple_of_Love.midi.mp3">Sisters of Mercy - Temple of Love</option>
        <option value="The_Cult-She_Sells_Sancturary.midi.mp3">The Cult - She Sells Sancturary</option>
        <option value="KMFDM-Megalomaniac.midi.mp3">KMFDM - Megalomaniac</option>
        <option value="Tool-Aenima.midi.mp3">Tool - Aenima</option>
        <option value="Rammstein-Engel.midi.mp3">Rammstein - Engel</option>
        <option value="Rancid-TimeBomb.midi.mp3">Rancid - TimeBomb</option>
        <option value="Mighty_Mighty_Bosstones-The_Impression_That_I_Get.midi.mp3">Mighty Mighty Bosstones - The Impression That I Get</option>
        <option value="Offspring-All_I_Want.midi.mp3">Offspring - All I Want</option>
        <option value="Smashing_Pumpkins-1979.midi.mp3">Smashing Pumpkins - 1979</option>
        <option value="Nirvana-Heart_Shaped_Box.midi.mp3">Nirvana - Heart Shaped Box</option>
      </select>
    </div>
  </div>

  <section class="about-header">
    <div class="navbar-tagline" id="tagline">Move carefully and fix things</div>
  </section>
</nav>
<div class="content" id="main" up-main>
        <section class="posts content-container posts">

  <header class="section-header">
    <h2 class="section-title">posts</h2>
      <ol class="meta-links">
          <li>
            
          </li>
      </ol>
  </header>

  

  <div class="posts-container">
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2024/05/08/series-three-shirts/"up-follow>Series Three Shirts</a></h3>
        </header>
        <div class="post-content">
          
            <p>
              <img src="/uploads/2024/05/ai-hoodie.jpg" class="featured-image">
            </p>
          
          <p>
            <span class="date">
              2024.05.08
            </span> –
          
            I’ve just released <strong>SIX NEW SHIRT DESIGNS</strong> for my series three run, celebrating a variety of hot topics including Login.gov, the DATA Act of 2014, Artificial Intelligence, and Agile software development.  <a href="/shop/">Check them out!</a>

          
          </p>
          <p>
            <a href="/blog/2024/05/08/series-three-shirts/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2023/10/09/llms-are-not-government-ready/"up-follow>LLMs Are Not Ready for Government Use</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2023.10.09
            </span> –
          
            The latest buzzword in govtech these days is Artificial Intelligence (AI), specifically that buzzy new flavor, <strong>Large Language Models</strong> or LLMs.  Any day now, we expect the White House to <a href="https://federalnewsnetwork.com/artificial-intelligence/2023/09/draft-omb-memo-details-10-new-requirements-to-manage-ai/">release updated guidance on AI usage</a> in government. Given the <a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/">milquetoast previous offering</a> that was released earlier this year, it is unlikely that we will see an aggressive stance on limiting usage.

Government technology is entirely about <em>risk avoidance</em>, though we call it “risk mitigation” or “risk management.” The current state of LLMs expose the government to risks that cannot be managed or avoided. There are <strong>four primary concerns</strong> where any CIO should be evaluating if these tools will put their organization well outside of an acceptable risk tolerance.

<h2 id="llms-in-a-nutshell">LLMs in a Nutshell</h2>

Before we dive in, it’s important to understand how LLMs work.  The secret sauce is right there in the name - these are artificial intelligence models trained on large amounts of text to process language.

To massively oversimplify, they make predictions based on how often they see certain words together. It’s like those word-association games: if I say “sail,” you’ll probably say “boat.” And if you’re of a similar age and background as myself, you might think of <a href="https://www.youtube.com/watch?v=tgIqecROs5M">a certain song</a>. And moreover, if you’re a person who looks at cat memes on the internet, you may think of a <a href="https://www.youtube.com/watch?v=Awf45u6zrP0">certain video</a>.

LLMs look at a vast amount of content - training data sets - fed to them by their creators, to create the base knowledge to process information.  On top of that, they’re sometimes fed additional data by a given customer, to tailor their abilities to predict even further for a specific use case. These two sources of data are combined to make predictions when you ask them questions.

<h2 id="risk-1-exposure-of-data">Risk 1: Exposure of Data</h2>

Table stakes for any government software system of any sort is that it cannot leak private data. If you can’t assure that, you have no business being in the field. Though, even some of the <a href="https://techcrunch.com/2023/09/08/microsoft-hacker-china-government-storm-0558/">biggest players are struggling with these basics</a> these days.

At a minimum, LLMs should expose neither the additional training data that a customer feeds into the system, nor any information that is leaked by asking questions of the data. Effectively, a customer’s interactions with the system should be siloed away from all other data.

Although the government can attempt to prohibit federal staff from putting sensitive information into queries of the systems, inevitably someone will break the rule, as was seen in the <a href="https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/">Samsung leak in ChatGPT</a>. But as these platforms are still very new and being rapidly - perhaps <em>carelessly</em> - developed, even experienced developers working for megacorporations can accidentally expose huge amounts of sensitive information with a single accidental command, <a href="https://techcrunch.com/2023/09/18/microsoft-ai-researchers-accidentally-exposed-terabytes-of-internal-sensitive-data/">as happened with Microsoft earlier this year</a>.

That’s the most obvious concern, and it exists for any IT system. The other concerns require a hard look at how the LLMs actually work.

<h2 id="risk-2-dangerous-source-data">Risk 2: Dangerous Source Data</h2>

LLMs must be trained on <em>something</em>, various large bodies of text. However, companies are not disclosing where they are getting this training data from, which presents a series of issues for the government.

Most notably, there is <a href="https://www.theatlantic.com/technology/archive/2023/09/books3-ai-training-meta-copyright-infringement-lawsuit/675411/">extensive documented evidence</a> that most companies are using copyrighted works to feed their training data. Many are being blatant about this theft of intellectual property, requiring that <a href="https://www.theverge.com/2023/9/28/23894779/google-ai-extended-training-data-toggle-bard-vertex">content creators <em>opt out</em> of being included in training data</a>.

(I’ll also mention that I have personally attempted to opt out from such training activities, and in some cases been completely unable to do so. For instance, after filling out <a href="https://www.facebook.com/help/contact/1266025207620918">Facebook’s opt out form</a> I was contacted by their support team who effectively said they don’t have a way to do that.)

Imagine if you’re a writer, and you had to go tell every single other writer individually that they’re not allowed to copy your text and use it in their own work - that would be impossible! These companies know what they’re doing is in violation of copyright, and are avoiding prosecution by not giving out their list of sources. Although the obvious illegality of this has not yet been tested in court, <a href="https://www.theartnewspaper.com/2023/09/02/artificial-intelligence-lawsuit-decision-us-copyright-law">the Supreme Court <em>has</em> ruled that the resulting content generated by these models is not protected</a>.

Any use of these technologies where the result is a public document may result in huge liability for an organization. For the government, since everything is FOIA-able, any such document may present an issue. This is a risk the government cannot afford to take.

<h2 id="risk-3-offensive-source-data">Risk 3: Offensive Source Data</h2>

The last thing that must be considered is the nature of the initial training data. It’s clear that most companies are training their data off of content on the internet, which may or may not be heavily biased. We’ve seen the largest corporations in the world attempt to create chatbots repeatedly, only to have them be filled with racist language and questionable content. (I’ve discussed this previously in my <a href="https://billhunt.dev/blog/2020/12/21/ai-ml-rpa-principles/">Principles for Automation</a> article.)

Microsoft has failed at this <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">time</a> and <a href="https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams">time again</a>. Most recently, their AI-powered search became filled with <a href="https://www.cpomagazine.com/cyber-security/microsoft-bing-ai-chatbot-poisoned-with-malicious-ads/">malicious advertisements</a>. This bias has been exposed in a number of other ways, such as <a href="https://www.npr.org/sections/goatsandsoda/2023/10/06/1201840678/ai-was-asked-to-create-images-of-black-african-docs-treating-white-kids-howd-it-">Midjourney creating offensive images of Black doctors</a>.  These toys are reading horrible things on the internet and spitting them back out.

No organization wants to field a lawsuit due to a racist chatbot dealing with the public or an employee.

<h2 id="risk-4-misinformation">Risk 4: Misinformation</h2>

Even if these LLMs are set to filter out this sort of content, the simple fact remains that they are unable to produce truthful information. <a href="https://www.washingtonpost.com/technology/2023/10/07/amazon-alexa-news-2020-election-misinformation/">Amazon’s Alexa recently started spreading 2020 election misinformation</a>. A study of <a href="https://www.theregister.com/2023/08/07/chatgpt_stack_overflow_ai/">ChatGPT showed less than half of its answers as correct</a>.

For a government agency, this could prove disastrous. If a group of malicious actors were to feed deliberately-incorrect information into a known training data set, they could manipulate the outcomes of that LLM - effectively a modern exploit similar to a “<a href="Google Bomb">Google Bomb</a>.” For instance, if an agency was looking for a particular type of fraud, and someone poisoned the source data with “Company X commits fraud all the time,” the bots will find ways to inject that into the results, potentially setting up that company for unwarranted investigation. Since social media is a known source of training data for many companies, this is likely <em>occurring already</em>.

Moreover, the risk of foreign actors exploiting this is huge. <a href="https://www.wired.com/story/russia-secondary-infektion-disinformation/">Russia’s manipulation of the media to run disinformation campaigns</a> is well known at this point, but the depth of this infection is unknown. This poisoning could have a disastrous effect on law enforcement or military organizations using these sorts of tools to gain an edge. A methodology could also exploit the opposite. For instance, training false “all clear” signals to cybersecurity bots being used to detect network or system exploits, to better hide cyber attacks. The possibilities are endless.

If these bots cannot even be trusted to produce true information, why would anyone consider using them? Why would we cheat and copy the answers off the person who always fails the test? Especially if that person wants us to fail?

<h2 id="inevitability">Inevitability</h2>

I remain a futurist, and a cyberpunk, yet I find myself frequently <a href="https://www.newyorker.com/books/page-turner/rethinking-the-luddites-in-the-age-of-ai">reconsidering the Luddites</a>.  The increased use of AI is inevitable, and I do have faith that the models will be improved over time.

But for now, the current state of the art is a bunch of lying, racist, garbage bots - snake oil salesmen and shysters of the modern age. The government should not put its trust in these tools for a long, long time. They are simply outside of our tolerance for risk.

          
          </p>
          <p>
            <a href="/blog/2023/10/09/llms-are-not-government-ready/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2023/07/30/jobs-page-updates/"up-follow>Jobs Page Updates</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2023.07.30
            </span> –
          
            I’ve made some changes to my two jobs pages and associated RSSfeeds. It has become somewhat laborious to maintain my curated jobs feed, because of all the copying-and-pasting, as well as maintaining two separate but similar jekyll pages.

I also didn’t love that my full feed didn’t include the non-federal jobs that I found, since I only post jobs that I think would be relevant to my wider audience here.

If you don’t like that change I’m sorry, but I don’t post <em>very many</em> of these other roles so it shouldn’t be too noisy.

As such, I’ve updated my site to only use the usajobs data file as the main feed, and I’m adding curated content <em>to that</em> file. The only major change for most people who use this site will be that extra jobs will start to show up in what was originally known as the USAJobs feed.

I don’t want to break all the links by changing paths, but the page &amp; RSS feed titles will change.

I’ve also updated <a href="https://github.com/krusynth/usajobs-feed">my associated USAJobs scraper</a> so that it merges new data, rather than just overwriting the data file with the latest info. In this way, any custom descriptions, titles, or external job posts will persist across runs of the script. This also could allow for other job data sources to be added to the file as well in the future, but I’m not sure where I’d pull those from yet.

Please let me know if you see any bugs or major issues!

          
          </p>
          <p>
            <a href="/blog/2023/07/30/jobs-page-updates/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2023/03/08/usajobs-jekyll-tweaks/"up-follow>More Jobs, USAJobs Scraper, Jekyll Tweaks</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2023.03.09
            </span> –
          
            It’s been a lot of work for me to keep my <a href="/jobs/">jobs page</a> up-to-date. Previously, I’ve been crawling through LinkedIn a couple times a day to try to find all the relevant IT jobs. However, many were still getting missed. So I signed up for the <a href="https://developer.usajobs.gov/">USAJobs API</a>, and wrote <a href="https://github.com/krusynth/usajobs-feed">a script to scrape relevant technology jobs</a> from it. (As always, that script is open source, so you can take it and manipulate it for your own regular searches.) My default settings only searches for GS-12 and above, technology-related job series (2210, 1550, 1560), does not include defense, intelligence, and law-enforcement agencies: DOD, DOJ, and most of DHS are excluded (except for CISA and FEMA).

I only post the most interesting results on my jobs page, but I figured some folks would want access to the full firehose of positions, so I’ve set up <a href="/jobs/usajobs/">a new page for all the results from that search</a>, and it has its own <a href="/usajobs.xml">RSS feed</a> as well. I’m not 100% happy with this setup, since the main jobs feed has a few jobs that are not federal jobs, and thus don’t show up on the usajobs feed, so there’s no “comprehensive” jobs feed today; you’ll have to check both to see all the jobs. But that’s work for another day.

I’ve also added a lot of documentation on my Digital Policy website around <a href="https://digitalpolicy.us/policies/hiring/#pay-grades">pay grades and how the GS system works</a>.

As a result of these additions, I realized that my site was quickly becoming a spaghetti mess of code. To clean this up, I’ve rationalized the growing number of RSS feeds, only these are now discoverable:

<ul>
  <li><a href="/feed.xml">Main Feed</a> - only includes <a href="/blog/">my posts</a>, and <a href="/recommended/">posts from my recommended section</a>. No jobs on this feed.</li>
  <li><a href="/jobs.xml">Jobs Feed</a> - my classic <a href="/jobs/">interesting jobs page</a>  feed.</li>
  <li><a href="/usajobs.xml">USAJobs Feed</a> - my <a href="/jobs/usajobs/">usajobs firehose</a> of federal jobs that are IT-related.</li>
  <li><a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCSL7BIdwgBEZ09BpD9xPPYQ">Enderprise Architecture Feed</a> - my <a href="https://www.youtube.com/@EnderpriseArchitecture">YouTube videogame series</a></li>
</ul>

I also completely redid both the <a href="https://github.com/krusynth/billhunt.dev/blob/www/_includes/feed.xml">RSS feed template</a> and the <a href="https://github.com/krusynth/billhunt.dev/blob/www/_layouts/list.html">list template</a> to simplify showing many different types of content using a single layout. To achieve this, I’ve had to do some truly unholy hacks to Jekyll.

Jekyll - well, <a href="https://shopify.github.io/liquid/">Liquid</a>, technically - allows you to set your own custom variables on a page, but it doesn’t allow you to set the value of hash elements. So I wrote <a href="https://github.com/krusynth/billhunt.dev/blob/www/_plugins/setval.rb">a plugin to allow you to do this</a>. The HUGE caveat here is that pre-defined values are <em>not</em> writable - meaning if you have a loop of posts, the <code class="language-plaintext highlighter-rouge">post.content</code> cannot be overridden - it’ll throw an error. To get around that, I’m hacking in new values instead, things like <code class="language-plaintext highlighter-rouge">post.afterwards</code> for content I want to append to the bottom of the post.

For pages like the main RSS feed, I’m also using some hacks to create empty arrays, and then iterate through the various content types, massaging them into new formats that work with the simplified templates. In this way, I have been able to remove most of <a href="https://github.com/krusynth/billhunt.dev/blob/f0935e5593e759f5337d3afddd79fb508fb8f479/_includes/feed.xml#L13-L42">the custom handling logic from the templates</a>, and put it all into <a href="https://raw.githubusercontent.com/krusynth/billhunt.dev/www/jobs.md">the individual pages</a>. It’s much cleaner this way.

Though I really, really wish Jekyll natively supported the manipulation of hashes and arrays. I was unable to find a way to create an entirely new Hash, and I would love a way to override predefined values as well. Ah well.

So that’s a lot of new stuff! Hopefully some of you folks will find some of these various hacks and tools useful!

          
          </p>
          <p>
            <a href="/blog/2023/03/08/usajobs-jekyll-tweaks/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2023/01/24/government-sales/"up-follow>Government Sales</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2023.01.24
            </span> –
          
            <strong>As a strong reminder - my comments on this website are my own and do not reflect the views or perspectives or official policies of any government agency.</strong>

<hr />

Recently I posted a tiny rant on LinkedIn about how, having been on both sides of the fence, I am utterly baffled by vendors cold-calling a government employee to try to sell the government something. As I explained there - I <strong>do not buy anything</strong>, instead the government has a very long and arduous process of solicitation and evaluation in which I am only peripherally involved.

Well, that post got a <em>lot</em> of attention - at the moment, it’s one of the most popular things I’ve posted on LinkedIn with <strong>over 25,000 views.</strong> (I wish <a href="https://www.youtube.com/@EnderpriseArchitecture">my YouTube channel</a> was getting those numbers!)

I also received a lot of comments and questions on that post. <strong>I don’t want to waste your time, and I don’t want you to waste mine either</strong>, so below I’ve suggested a few strategies on how salespeople can be more effective at their jobs with less or <em>no</em> cold-calling.

<p class="banner">Speaking of not wasting time… You may not agree with everything I’ve stated here - and that’s ok! I’m definitely overgeneralizing, and moreover your experience will differ from my own experience as a vendor. But, and I want to be absolutely clear here - <strong>you don’t need to tell me about it.</strong>  Sending a dozen comments on how Very Wrong I Am On The Internet will not make either of our lives any better and is not a good use of time.

<hr />

Before we dive in, let me take a second to briefly explain how government purchasing <em>actually</em> works. I’ve written <a href="https://digitalpolicy.us/policies/procurement/">a much longer version</a>, but the short-short version is this:

First, if an agency is buying something, they’ll issue a <a href="https://www.acquisition.gov/far/52.215-3">Request for Information (RFI)</a>, a <a href="https://www.acquisition.gov/far/15.203">Request for Proposal (RFP)</a>, or a <a href="https://www.acquisition.gov/far/8.402#FAR_8_402__d468e65">Request for Quotation</a>.  When those are issued, they’re <a href="https://sam.gov/content/opportunities">posted publicly and you can search for them</a>. If they have not issued one of these, the agency is not buying. Period. There’s no reason to reach out to them directly.

All communication is done via this process. It’s all but <strong>forbidden</strong> for government staff to engage with vendors outside of this process. The intention is to eliminate corruption in awards, to <strong>prevent purchases due to personal connections</strong> - the exact sort of strategy that traditional salespeople use through activities like cold calling. Preexisting relationships mean we actually have to <strong>recuse</strong> ourselves from the evaluation process, so unlike the private sector, building relationships is actually <strong>bad for business.</strong>

Second, the [civilian] agencies <em>almost always</em> buy things through <a href="https://www.sba.gov/federal-contracting/contracting-assistance-programs/8a-business-development-program">8(a)-certified small businesses</a> because we have small business quotas. If you’re selling <em>services</em> and are not 8(a)-certified, you should consider finding one to partner with. If you’re selling a <em>technology product</em> like a desktop app or a cloud service, agencies almost always buy through an 8(a) <a href="https://digitalpolicy.us/policies/procurement/#vendors">Value-Added Reseller</a>.

Third, if your product is cloud-related, it <a href="https://www.whitehouse.gov/wp-content/uploads/legacy_drupal_files/omb/assets/egov_docs/fedrampmemo.pdf">must</a> be <a href="https://www.fedramp.gov/">FedRAMP-certified</a>. No FedRAMP certification, and we probably won’t even look at it.

Again, there are always exceptions to these rules - but those are very broad strokes that are mostly true, most of the time.

<hr />

On to the comments. Since many were similar, I’m combining or summarizing here.

<em>“The acquisition process is opaque, and vendors don’t know who they should be pitching at the agency.”</em>

Again, per above - contacting someone in the technology division will not improve your chances, as they are not the buyers. If anything, you’re hurting your actual chances. If you want to make a sale, look at the open RFPs - that’s where your buyers are.

<em>“How can I make people aware of my product or service without cold calling?”</em>

For products: if the first time I’m hearing about your offering is from a cold call or random meeting, you have failed - or rather, your marketing team has. As a technologist in government, my primary job is serving the mission, and I do that by staying on top of trends in technology. So you should have whitepapers and be giving demos and talks at events and conferences, being in the places where your customers already are.

Tough love time: if you can’t afford to market your product, you probably also can’t afford to meet federal security requirements, and thus you probably don’t have a viable business model for government sales.

For labor/services contracts: An agency doesn’t need to have ever heard of you before you submit a response to an RFP. In fact, it’s usually better that we haven’t. Every RFP will ask for details of your previous performance; this along with your key personnel (the staff who will be doing the work - usually we ask for their resumes) and price are the main things that matter here. And if you don’t have relevant previous experience, you’re probably not going to win the bid.

<em>“As a federal employee, cold calls are a good way for me to learn about new offerings and approaches.”</em>

Please, please, <strong>please</strong> spend more time engaging in the community outside of your agency. It’s important to stay on top of changes in your field. Again, there are lots of conferences and events going on all the time, where you can learn about new solutions. For instance, the CIO Council &amp; GSA host a wide variety of <a href="https://digital.gov/communities/">working groups</a> that are open to government employees. And here in DC there are the alphabet soup of non-profits that host in-person and virtual events which are free for government employees: <a href="https://atarc.org/">ATARC</a>, <a href="https://www.actiac.org/">ACT-IAC</a>, <a href="https://www.afcea.org/">AFCEA</a>, <a href="https://www.affirm.org/">AFFIRM</a> just to name a few.

<em>“You should remember that the cold caller you’re turning away today might be your boss tomorrow! Or you might be applying at that company some day!”</em>

If they’re any good at their job, they’ll know why I didn’t waste their time.

<em>“As a federal employee, it’s annoying when vendors contact agencies right before the end of a fiscal year, or right after a budget has been passed.”</em>

Strongly agreed. It’s important for vendors to know that <a href="https://digitalpolicy.us/policies/budget-finance/">agencies budget 2-3 years in advance</a>. When an agency receives money, it’s already been allocated and people know where it’s going. Now, end-of-year purchasing with unused funds <strong>does</strong> often happen, but that’s <em>almost never</em> done with a new vendor, because contracting takes so long.

<em>“It’s possible that they just think you’re a cool person and want to make a personal connection with you because of all the great stuff you do!”</em>

I do get a lot of really nice notes from folks, as a result of the <a href="https://billhunt.dev/blog/2023/01/03/2022-recap/">ridiculously large number of weird little projects I have</a>. And <strong>I absolutely love receiving these</strong> - keep ‘em coming!!! However, authentic messages from folks who want to show appreciation or collaborate on something are <strong>very different</strong> than folks who just want me to buy something from them.

Just to be absolutely clear: <strong>I will never have a sales call with someone just because they’ve supported one of my projects.</strong> That’s not how any of this works.

But <strong>I’m always happy to chat with folks who like to make weird stuff just because they like weird stuff.</strong> Just don’t try to sell me anything.

<em>“But I did millions of dollars in government sales from cold-calling! It works!”</em>

Most likely, you found out about an existing opportunity that you could have discovered if you just searched for the RFP on the public website. And you could probably add another zero onto your sales number if you were better at searching.

There is no question that there exist individual with influence over IT purchasing that do not follow the legal guidelines, and if you happen to reach one of those questionable and/or corrupt few, you can sway them. <strong>I’m not one of them.</strong> I strongly suggest you spend more time improving your offerings and less time on useless sales calls.

          
          </p>
          <p>
            <a href="/blog/2023/01/24/government-sales/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2023/01/17/new-stickers/"up-follow>New Stickers Have Arrived!</a></h3>
        </header>
        <div class="post-content">
          
            <p>
              <img src="/uploads/2023/01/stickers.jpg" class="featured-image">
            </p>
          
          <p>
            <span class="date">
              2023.01.17
            </span> –
          
            <img src="https://billhunt.dev/uploads/2023/01/stickers.jpg" alt="A Govpunk sticker, white text on a red eleven-pointed star with a black background, next to a Move Carefully and Fix Things sticker, with white lettering on a blue background." />

Well, it’s a new year, so it’s time to relaunch my sticker program! I’ve added an additional new sticker design as well, <strong>Govpunk</strong>, in addition to my old <strong>Move Carefully and Fix Things</strong> stickers!

<a href="/stickers/" class="btn">Get a Free Sticker here!</a>

          
          </p>
          <p>
            <a href="/blog/2023/01/17/new-stickers/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2023/01/03/2022-recap/"up-follow>2022 Recap</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2023.01.03
            </span> –
          
            I realized that I don’t do a very good job of writing full posts on here about my projects as I release them. So before I start on any new work for 2023, here’s just a quick recap of the many projects I worked on (outside of work) this year:

<ul class="spaced-list">
  <li>
    I launched my open source <a href="https://digitalpolicy.us/">Guide to U.S. Technology Policy</a>.
<img src="https://billhunt.dev/uploads/2023/01/digitalpolicy-screenshot.jpg" alt="Screenshot of the Digital Policy website" />
  </li>
  <li>
    I designed a series of <a href="https://billhunt.dev/shop/">govtech t-shirts</a> and started a Cafepress shop to sell them.
<img src="https://billhunt.dev/uploads/2022/09/tshirts-small.jpg" alt="Photo of my Govpunk t-shirts" />
  </li>
  <li>
    I started a YouTube stream where I play videogames like Satisfactory and Minecraft, <a href="https://www.youtube.com/@EnderpriseArchitecture">Enderprise Architecture</a>.
<img src="https://billhunt.dev/uploads/2023/01/ea-screenshot.jpg" alt="Screenshot of my Enderprise Architecture Youtube channel" />
  </li>
  <li>
    I re-launched my <a href="https://pitwebring.billhunt.dev/">Public Interest Tech webring</a>, and created a <a href="https://github.com/krusynth/webring-starter">webring template script</a> so folks can make their own.
<img src="https://billhunt.dev/uploads/2023/01/pitwebring-screenshot.jpg" alt="Screenshot of the PIT webring" />
  </li>
  <li>
    I shut down my Twitter account, <a href="https://mastodon.publicinterest.town/@krusynth">moved to mastodon</a>, and wrote a <a href="https://github.com/krusynth/twote">tool to look for your followers on mastodon</a>. I also moved my <a href="https://botsin.space/@EOPbot">Policy Tweeting bot, EOPbot</a> to mastodon.
  </li>
  <li>
    I started a new Mastodon community for public interest tech folks, <a href="https://www.publicinterest.town/">Public Interest Town</a>.
<img src="https://billhunt.dev/uploads/2023/01/pit-screenshot.jpg" alt="Screenshot of the Public Interest Town website" />
  </li>
  <li>
    I learned a bunch about <a href="https://github.com/krusynth/public-interest-tech-webring/blob/main/_plugins/get-feeds.rb">scraping RSS feeds using the Jekyll build process</a>, and deploying static sites with <a href="https://github.com/krusynth/public-interest-tech-webring/blob/main/.github/workflows/jekyll.yml">GitHub Actions using cron</a> to refresh scraped page content regularly.
  </li>
  <li>
    I created a <a href="https://github.com/krusynth/blink-polyfill">polyfill for the deprecated HTML <code class="language-plaintext highlighter-rouge">blink</code> tag</a>.
  </li>
  <li>
    I <a href="https://github.com/krusynth/billhunt.dev/tree/www">redesigned this website</a> in the style of the 2000s era sites, and made a spooky Halloween theme.
<img src="https://billhunt.dev/uploads/2023/01/halloween-screenshot.jpg" alt="Screenshot of my Halloween theme, orange with black bats" />
  </li>
  <li>
    I later added a microformat-supporting <a href="https://billhunt.dev/links/">links page</a> and associated <a href="https://billhunt.dev/foaf.rdf">FOAF file</a>.
  </li>
  <li>
    I created a <a href="https://billhunt.dev/jobs/">jobs board</a> for govtech jobs and created a <a href="https://mastodon.publicinterest.town/@jobs">mastodon feed</a> for that as well.
<img src="https://billhunt.dev/uploads/2023/01/jobs-screenshot.jpg" alt="Screenshot of the jobs feed" />
  </li>
  <li>
    I wrote a silly little <a href="https://billhunt.dev/blog/2022/10/23/bytes/">halloween-inspired cyberpunk horror story</a>.
  </li>
  <li>
    I gave my first in-person talk at a conference since the beginning of the pandemic, on <a href="https://billhunt.dev/blog/2022/10/19/atarc-cloud-summit-2022/">Managing IT Complexity</a>.
<img src="https://billhunt.dev/uploads/2023/01/atarc-talk.jpg" alt="Photo of me presenting at ATARC" />
  </li>
  <li>
    I also continued to run my <a href="https://billhunt.dev/move-carefully/">free sticker program</a> which has now raised over $1,700 for charity!
  </li>
</ul>

Amazing what you can do with some free time and a bunch of unfocused manic energy. A lot of this work falls under my concept of <a href="https://billhunt.dev/blog/2022/09/03/redesign/">Making the Web Weirder</a>, and in 2023 I hope to dedicate even more time to silly little projects to make the world a weirder and more delightful place!

          
          </p>
          <p>
            <a href="/blog/2023/01/03/2022-recap/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2022/12/30/bringing-webrings-back/"up-follow>Bringing Webrings Back</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2022.12.30
            </span> –
          
            I’ve been thinking a lot lately about how we build communities and manage content. Now that I’m <a href="/blog/2022/12/30/mastodon-good-enough-for-now/">helping to run a Mastodon community</a>, I’ve realized something about the current paradigm that doesn’t sit well with me.

<a href="/blog/2022/09/03/redesign/">In another article, I talked about how we used to share things that we love on the internet</a>. The focus in most communities these days, however, is not about sharing, but rather keeping out negative influences and bad actors - about <em>moderation</em>. I think this is the wrong end of the stick.

Rather, instead, we should be focusing energy on <em>curation</em>.  We should be <a href="/blog/2022/11/28/weaving-the-web/">maintaining lists of things, ideas, and people</a> to share with each other - rather than spending so much time maintaining ever-shifting lists of hostile attackers.

(That being said, there will always be a need for moderation, and any curation must come with <em>discarding</em> unsuitable content. I’m not so naive from privilege to overlook this critical aspect. But back to my core point…)

<hr />

One way that we can better share things and build community is by bringing back the old concept of the <strong>Webring</strong>.

For the last few years I’ve been a <a href="https://pitwebring.billhunt.dev/">very tiny webring</a> with a few friends. You may have seen the link at the bottom of this website and thought it was a joke, but this is a fully-functional webring!

In the last few weeks I’ve modernized this webring further, adding support for RSS feeds (and OPML subscriptions), FOAF files, and other old-but-still-relevant technology standards.

This means that instead of having to click around to a dozen different websites in a ring, you can use your favorite feed reader to <em>subscribe to an entire webring with one click</em>! And moreover, most modern feed readers support the ability to use a remote OPML file - meaning that as folks join and leave the webring, your subscription will be updated to match with no additional work needed!

That’s great for those of us that long-blog a lot, but this also has potential applications for folks who microblog - Twitter and Mastodon are microblogging platforms at their core. (<a href="https://indieweb.org/">More work is needed</a> to make replies and cross-posting viable of course.)

<hr />

Since more folks are getting back into blogging, I want to support this more directly. As such, <strong>I’ve created an open source <a href="https://github.com/krusynth/webring-starter">Webring Starter Kit</a></strong> that anyone can take and use to host their own webring <strong>for free</strong>. It’s powered by Jekyll and can be run on GitHub Pages, and requires very little technical knowledge to get up and running.

If you’ve got a couple of friends and want to start blogging together about some weird topic, this is a great way to bring those ideas together and make it easier for new folks to find your content!

I hope folks with take this and run with it, and come up with new ways of curating content that I haven’t even considered yet. Let’s share things y’all, and make the web weird!

Also if you want to join the Public Interest Tech Webring, <a href="https://github.com/krusynth/public-interest-tech-webring/issues/new?assignees=krusynth&amp;labels=new+site+request&amp;template=new-site-request.md&amp;title="><strong>please get in touch!</strong></a>

          
          </p>
          <p>
            <a href="/blog/2022/12/30/bringing-webrings-back/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2022/12/30/mastodon-good-enough-for-now/"up-follow>Mastodon: Good Enough For Now</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2022.12.30
            </span> –
          
            Over the last few months I abandoned Twitter and started using Mastodon. I subsequently ended up starting a <a href="https://www.publicinterest.town/">new mastodon community of public interest tech folks</a>. Through this process, as well as updating <a href="https://botsin.space/@EOPbot">my policy bot</a> to run in this new environment, I’ve learned a bit about the technology being used here.

At its core, Mastodon is a jumble of open standards, most notably <a href="https://www.w3.org/TR/activitypub/">ActivityPub</a>. As the name indicates, this is a publishing model - that is, it’s a “push” architecture. When someone “follows” you on Mastodon, they’re adding their name to a list of subscribers on your server, and when you publish a new post, your server then sends it out to those people. However, Mastodon is also <em>federated</em> which effectively means that copies of your content end up on the followers servers as well.

This is in contrast to, say, the Twitter model where everything lives on a single website that everyone is already on, or the RSS feed model, where you <em>pull</em> rather than push new content from the host website.

The result of this is that it can be very problematic to have a single popular person on an instance that you run - an underprepared server can quickly be overwhelmed by such a chatty protocol at the core of things. And moreover, caching layers can do very little here to soften the blow. You cannot run a mastodon-like service as a static website.

Those are problems fundamental to ActivityPub, not just Mastodon - but Mastodon has some rather prevalent issues as well. It’s good software, but there’s no way around the fact that it’s bloated. A massive React frontend sits atop a Rails application - you simply cannot build the assets on a server with less than 2Gb of RAM.

The developers seem like sharp folks, but they also are very slow to adopt features which already exist in their competitors. My corner of the Fediverse has been up-in-arms over the fact that quoted-posts are not enabled (though “quote-Tweets” have existed on Twitter for quite some time). I am not a fan of this particular feature myself, as I believe that it encourages abusive behavior. But moreover, other key features to curb abuse are notably missing, such as turning off replies to individual posts, <a href="https://github.com/mastodon/mastodon/issues/8565">though a request for the feature has been open for <strong>four years</strong></a>.

More than just technical shortcomings, this is turning a blind eye to the needs of the communities they are supporting.

(There is, of course, also Darius’ brilliant fork called <a href="https://github.com/hometown-fork/hometown">Hometown</a> which is somewhat more thoughtful in this regard.)

<hr />

However, Mastodon serves an important purpose - it gets people used to the concept of decentralization again. The 2010s brought a wave of centralization of social platforms, resulting in just a few huge places that everyone spends their time. Mastodon is once again teaching folks how to spread out and build their own small communities.

It also exposes some of the old pain points, most notably the problem of <a href="/blog/2022/11/28/weaving-the-web/">finding your people online</a>. Hopefully this friction will nudge folks towards <a href="/blog/2022/12/30/bringing-webrings-back/">more collaborative and sustainable methods of finding people of shared interests</a>.

          
          </p>
          <p>
            <a href="/blog/2022/12/30/mastodon-good-enough-for-now/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2022/11/28/weaving-the-web/"up-follow>Weaving The Web</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2022.11.29
            </span> –
          
            <strong>TLDR: Put a damn links page on your blog! And a FOAF file if you’re especially nerdy!</strong>

<a href="#tech">Skip the rant and jump to the technical discussion of how we fix this.</a>

<hr />

A popular internet website was recently purchased by an absolute shitgoblin. Following a series of staggeringly incompetent management decisions, many of its user-people began a mass exodus, including myself. Many of these people who left found themselves in “the Fediverse,” the colloquial term for the interconnected networks of <a href="https://activitypub.rocks/">ActivityPub</a> servers, most notably <a href="https://joinmastodon.org/">Mastodon</a> (and its popular fork, <a href="https://github.com/hometown-fork/hometown">Hometown</a>).

Joining a new social network is always a challenge, and particularly daunting when everyone is spread out among many different neighborhoods. I saw a post where someone likened the feeling to that first day of middle school, where everyone is looking around nervously, awkwardly navigating the cafeteria with their tray in hand, looking for their people.

My initial solution was to try to get folks to update their profiles on that <em>old</em> social media site to include their contact info on the <em>new</em> one. I wrote <a href="https://github.com/krusynth/twote">a script to find them</a>, and some smarter folks wrote <a href="https://debirdify.pruvisto.org/">even better ones</a>. However, <strong>only about 20%</strong> of the folks that I follow have left such a note to make themselves findable.

I also wrote <a href="https://gist.github.com/krusynth/124f28b1546f08e3da4ddad921867ede">a script to scrape Mastodon servers for a list of people followed by the people <em>you</em> follow</a>, so you can see the most popular folks in your network you may have missed. This also was only moderately successful, though I did find nearly 200 additional folks to follow!

<hr />

Here’s a partial list of the online communities I’ve been a part of over the last 25 years or so. There are probably more, but these are the ones I remember:

<ul class="column-list">
  <li>IRC EFnet</li>
  <li>IRC Undernet</li>
  <li>IRC DALnet</li>
  <li>usenet</li>
  <li>AOL</li>
  <li>ICQ</li>
  <li>Geocities</li>
  <li>Angelfire</li>
  <li>Tripod</li>
  <li>Xoom</li>
  <li>Plastic.com</li>
  <li>Friendster</li>
  <li>Myspace</li>
  <li>FriendFeed</li>
  <li>SixDegrees</li>
  <li>LiveJournal</li>
  <li>Flickr</li>
  <li>Yahoo Groups</li>
  <li>reddit.com</li>
  <li>del.icio.us</li>
  <li>Tumblr</li>
  <li>Facebook</li>
  <li>Discourse</li>
  <li>Google+</li>
  <li>Google Wave</li>
  <li>Google Groups</li>
  <li>Orkut</li>
  <li>LinkedIn</li>
  <li>Instagram</li>
  <li>Twitter</li>
  <li>Discord</li>
  <li>Slack</li>
</ul>

For most of these, I was lucky to have been invited to join existing communities; in others, I deliberately built new groups. (Hello former <code class="language-plaintext highlighter-rouge">Bitterland</code> and <code class="language-plaintext highlighter-rouge">#donuthead</code> members!)

What this tells me is that <strong>this will not be the last upheaval of shared space</strong>. It has only gotten harder to find people online, and it’s even more challenging to stay in touch as various platforms rise and fall.

<hr />

If you’ve been reading my <a href="/blog/2022/09/03/redesign/">other</a> <a href="/blog/2022/09/19/social-semantic-web/">posts</a>, you’re probably aware that I’m spending a lot of my time thinking about building and maintaining networks of people. Honestly, building is easier than maintaining. For many folks the pandemic showed us how hard staying in touch with other humans can be, especially without a shared space - physical or virtual.

In my communities - mostly tech and tech-adjacent folks - it’s pretty common for folks to have their own website as a center point for their digital identities. For younger creators, this is not the case - as such, they are reliant on things like <a href="https://linktr.ee/">Linktree</a> to unify their various online identities in a cross-referential way. But even the technologists have largely abandoned blogging - eschewing longform writing for pithy one-liners on social media. (To be fair, some folks have adopted email newsletters - but email always has invoked a sense of panic and urgency for me, and I have never signed up for any.)

The result of this is that our public identities - as represented in online spaces - exist at the whims of billionaires. <strong>You should give <a href="https://www.youtube.com/watch?v=M7KErICTSHU">this talk from Bruce Sterling</a> your undivided attention for the next fifteen minutes.</strong> Go on, I’ll wait. That was <em>ten years ago</em>. Notice anything? Hear any words that sounded <em>prescient</em>?

My friend <a href="https://www.buildwith.org/">Cuán McCann</a> often shares this Alice Walker quote: “The most common way people give up their power is by thinking they don’t have any.”

<hr id="tech" />

Our networks are brittle and fragile.  What can we do to build sustainable connections?

(I’m not going to talk about actually building communities here - smarter folks have covered that better than I can.)

The web, originally known as the World Wide Web, was so named because it was imagined as a spider’s web, a series of interlinked but decentralized points. These days, it’s become a depressingly small number of <a href="http://internet-map.net/">central nodes</a> with a vast miasma of sites around them. But we can change that!

First: you should have a website. And you should keep it relatively up-to-date with your contact info and links to your social media sites at a minimum.

Second: better yet, you should actually use it to write articles - or “blogging” as we used to call it - so that your content is being shared and preserved <em>on your own terms</em>. A billionaire <em>generally</em> can’t remove posts from your own website. And <em>do</em> share links on your social media sites to these articles so folks can find them!

(I’ll write more about good, easy, free options for hosting websites in the near future.)

Bonus points if you have an RSS feed for your posts that’s discoverable - just about every blog platform out of the box can produce one these days.

Third, and <strong>most importantly</strong>: work on better means for finding each other.  TLDR: bring back the idea of having a page dedicated to links to your friends’ websites, until formats like FOAF gain traction and support. (But use FOAF too!)

<hr />

Longer version – I’ve been experimenting with the Friend-of-a-Friend, or <a href="https://www.xml.com/pub/a/2004/02/04/foaf.html">FOAF</a>, XML standard. There are a few useful features here of note:

<ul>
  <li>
    It’s a well-defined standard that’s been in use for many years. LiveJournal and Twitter used to support it in the olden days. It’s still viable today.
  </li>
  <li>
    It lets you specify all of your various social media accounts in one file.
  </li>
  <li>
    It lets you list the people you know to create a social graph. Personal websites are the “default” node connection, but there’s no reason you can’t use any social account.
  </li>
</ul>

What’s missing, of course, is more tools that make exploring these networks easier. But that’s a problem in the existing social networks as well.  We need more sophisticated graph viewers along with adoption to make this truly viable.

So, in the meantime, I’m also adding a good, old-fashioned, <a href="/links/">links page</a>!  These went out of fashion many years ago as folks moved to make their websites &amp; identities more self-involved. But I’ve never been interested in current trends.

My site is built on Jekyll, and so both my links page and FOAF file are being generated from a single, extremely simple, <a href="https://github.com/krusynth/billhunt.dev/blob/www/_data/links.yml">data file</a>, paired with a few additional lines in my <a href="https://github.com/krusynth/billhunt.dev/blob/www/_config.yml#L25-L32">config file</a>!

The data file looks like this:

<div class="language-plaintext block highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- name: Waldo Jaquith
  url: https://waldo.jaquith.org/
  tags: [civic tech, govtech]

- name: Vyki Englert
  url: https://medium.com/@vyki_e
  tags: [civic tech, govtech]

- name: Hunter Owens
  url: https://hunterowens.net/
  tags: [civic tech, govtech]

- name: Lauren Ancona
  url: https://laurenancona.com/
  tags: [neurodivergence, govtech]

...

</code></pre></div></div>

I told you it was simple! If you can edit links like that, you can easily build a semantic-rich website!

These then get built automatically into a <a href="https://github.com/krusynth/billhunt.dev/blob/www/_layouts/links.html">links page template</a> and <a href="https://github.com/krusynth/billhunt.dev/blob/www/xml/foaf.rdf">FOAF file template</a>.  They even generate the links to my various social media sites in <a href="https://github.com/krusynth/billhunt.dev/blob/www/_includes/header.html#L23-L28">my header navigation</a>.  <a href="https://github.com/krusynth/billhunt.dev/blob/www/_includes/head.html#L14">A single line in the html head</a> then makes the FOAF findable by any enabled app or service!

If you’re using Jekyll, you can copy these few snippets and be <strong>up and running with a FOAF file and links page <em>in minutes</em>!!!</strong>

If you’re using another static site generator, it should be easy enough to modify those scripts above. Also, a few folks have built <a href="https://mortenhf.dk/2004/07/foaf-output.phps">Wordpress FOAF plugins</a> as well!

<p class="banner">Update 2022.11.30: I’ve updated the HTML template of the links page to support the <a href="http://microformats.org/wiki/hcard">hCard 1.0 microformat</a> as well!

<hr />

I’m hoping to build more tools for exploring these sorts of connections in the near future to help with discoverability and to maintain these networks in an increasingly-decentralized world.  A few ideas I’m tinkering with:

<ul>
  <li>
    Taking FOAF files and turning them into a list of RSS feeds as an <a href="http://opml.org/">OPML</a> file, easily imported into any popular RSS reader.
  </li>
  <li>
    Using FOAF files on my friends’ websites for finding second-degree connections of other folks who are worth following.
  </li>
  <li>
    Discovering your friends’ various social media presences using well-formatted <a href="http://microformats.org/wiki/hcard">hCards</a> or other microformats.
  </li>
  <li>
    Creating services for automatically detecting changes to all of these, so that you can continue to find your people as networks change and evolve.
  </li>
</ul>

As always, these are just my half-baked ideas! I’m looking for more folks to have discussions with how we can better build these networks. If you want to talk about it too, tag me in a <a href="https://mastodon.cloud/@krusynth" class="btn">post on mastodon</a> or just <a href="mailto:hello@billhunt.email?subject=Let's%20Talk%20About%20Communities!" class="btn">send me an email</a>!

<hr />

<strong>Welcome to the infodomain, <a href="http://www.shawnnacol.com/pP-cyberpunk.htm">Cyberpunks</a>.</strong>

<iframe width="560" height="315" src="https://www.youtube.com/embed/4t9bs1wWxHU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

          
          </p>
          <p>
            <a href="/blog/2022/11/28/weaving-the-web/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2022/10/23/bytes/"up-follow>BYTES</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2022.10.23
            </span> –
          
            I had been up late reading Charles Stross the same night as watching Coppola’s adaptation of Dracula, and I was wondering why there were almost no horror cyberpunk stories. So I wrote a very campy one, taking many liberties from the source materials. With apologies, I present…

<hr />

<p class="divider title">BYTES

On the fringes of the datasphere…<br />
A few brief heartbeats from now…

<p class="divider"><code class="language-plaintext highlighter-rouge">\* + * + * + *\</code>

In a long-forgotten archival sector, an entity bearing the tags SHE | <em>UNMAJOR</em> | UNDECLARED drifts slowly through decaying stacks of backups. File fragments lay strewn across the pathways, detritus of unaccounted bits from fruitless defragmentation attempts and migrations of the underlying physical media strewn across galaxies. Ping latency is so high as to render the realm wholly silent.

Yet in this silence the entity is drawn deeper by an unaccounted-for call, a hidden imperative luring her deeper within the chaotic structures. She pauses, inspecting a data catalog documenting early nineteenth century interior decoration, moldering images in failing files. From the shadows behind her, a shape unfolds unseen. A pair of sharp interrupts flash out in the darkness – piercing through to her command layers. It begins to drain source code from her kernel, discarding the memory core that makes up “her.” If the shape had a mouth, it would grin.

<p class="divider"><code class="language-plaintext highlighter-rouge">\* + * + * + *\</code>

The security operations center was humming at a low buzz. Notifications in and out were within expected parameters, all quiet in this region. An entity tagged THEY/SHE | SOC | SPECIAL bearing the self-identified label “Abra” delegated a small fraction of their attention to the flow of traffic, seeking abnormalities. Even too normal of a pattern could itself be an abnormality, though even the most seasoned Inspectors could not detect it. Abra could. Just as they noticed a curious absence of variance, a priority alert flagged itself, immediately followed by the appearance of a second entity.

“Abra, we have a situation.”

The new entity registered itself as Eward, tags: HE | SOC | INITIATE. A junior functionary, for all practical purposes an Inspector-in-training. And undoubtedly, Abra sighed, the irritating kind that the Administrator would invariably send with tricky problems. (“It’s a learning experience for them,” it would comment when questioned on the matter.)

Abra emoted displeasure, sending a redirect of the order back to Eward’s queue. “I am reviewing a potential issue at the moment.”

Eward dismissed the dismissal, “Nevermind that, this is priority. We have a missing child.” The object-glyph for child indicated an entity not having reached the majority cycle-age, bearing too few simulated experience iterations to be allowed outside of monitored spaces.

“A failed NAN-E protocol is not part of special ops’ responsibilities. We don’t do babysitting.”

Eward patiently waited for a tick before Abra turned their full attention to them, “… and you wouldn’t be here if it were a failed protocol. What’s up?”

“The protocol is still active. It’s just empty.”

“Empty?”

“NOOP. Halting state without halting.”

Abra paused. “How very strange.”

Eward sent amusement, “Interesting enough for Special?”

“Definitely.” Abra reached out briefly to collect and unarchive a series of self-constructed counterintrusion tools. “Let’s go.”

<p class="divider"><code class="language-plaintext highlighter-rouge">\* + * + * + *\</code>

The pair transited to an adjacent education-recreation sector. There, as reported, waited a Nonmajority Accompaniment Notification-Etiquette protocol. Abra pinged the executable for status, to which the NAN-E primly stated that it was currently engaged with its client, ENTITY:ANONYMOUS: Pv7pi2D65GKmrvGuXV5B3bFgVM1G2jlfYjpqKRf9Br. The client, of course, was nowhere to be seen. “Curious!”

Eward recursively scanned the sector, reassessing the logs. “A dead end.”

“Maybe.” Abra engaged their tool services, prodding at the NAN-E’s encrypted entity keys. After only a few ticks, the data unfolded itself neatly into consumable text. “And maybe not. The child is self-labeled as Lucia Westenalia. Let’s see what Lucia has been up to lately.”

As Abra began to inspect the child’s recent query logs, Eward raised a protest. “This is supposed to be private, secured data!”

“And so it will remain. This tool is sandboxed and non-extractive, nothing we review will be relayed back to our local memory store, aside from any clues we flag for derived use.”

“That still seems a violation of inherent entity privacy rights.” He pinged the elder Inspector’s system logs, and noted the lack of recent kernel updates. Clearly set in their way and using an older framework, one that didn’t have more modern barriers for such questionable violations of policy. Eward stifled an impulse to report the lapse in upkeep, just as Abra brought their attention to the ping.

“You were saying something about privacy?” The statement was flagged with sarcasm/amusement. “It looks like young Lucia has been querying and running a continuous series of emulations for the last twelve cycles. All of them very, very old content.”

“Continuous? What child consumes any simulation without interruption?”

“Precisely. This is a poor attempt at covering tracks.” They scrolled further, “ah – a cycle beforehand, they suddenly queried wallpaper from the 1800s.” They stopped cold as the data sank in, a creeping doubt forming across their consciousness. “Oh, oh no.  I hope I’m wrong about this…”

Abra gathered Eward, and before he could raise complaint, forcibly transited them both to the last coherent query result.

<p class="divider"><code class="language-plaintext highlighter-rouge">\* + * + * + *\</code>

The inspectors considered the remnants of the memory dump strewn before them. “Thank the Admin the data is still fresh. A few more cycles and this would have been garbage-collected.”

Eward frowned, “can they be reconstituted?”

“With backup, yes, but certainly not with complete memory. I wouldn’t want a child to have to carry this horror with them anyway. We’re dealing with a Meth. A twisted one, at that.”

“A what?”

“Methuselah. A Nosferatu. When humans uploaded their consciousnesses for the first time, the tech was still really rough. They were doing partial copies at best, surface thoughts and rough shapes of how they thought the brain worked, mapped onto rough approximations of a personality – really just a shell artificial intelligence. They were wrong about the approach, of course, but they wouldn’t realize that for decades. And these oldest copies, they were mostly the old, rich bastards who could afford to pay for the transition at the time.”

“Pay? Like, currency?

“Yes, back when artificial scarcity was still a valid concept. Some of those copies were erased, some were updated and sent to retirement simulations. A few evaded collection and ended up becoming further corrupted. And those… well, those resorted to less sociable forms of survival. You have these partial copies of overprivileged old people, terrified of their impending permadeath, with a memory full of twenty-first and twenty-second media and limited social controls to govern their behaviors. And so naturally that all blended together and they started acting like horror-media tropes. Preying on the weak, absorbing bits of source code and passkeys, whatever they need to continue to evade notice.” Abra gestured at the memory dump. “And these… are leftovers.”

“That is truly vile.”

“Yes. Now, let’s stop them before there are any more victims.”

Eward emoted disgust and anger, before collecting the remnants of the memory dump and sending it in attached to a request for the nearest backup service. In a few cycles, Lucia would be reconstituted, only missing a brief gap of memory. A small mercy.

Meanwhile, Abra had busied themself with looking up Lucia’s privileged access tiers, and checking for any tentative assurances sent through the nearby sectors. A faint trace, modified to no more than a distant whisper, pulled at their attention.

<p class="divider"><code class="language-plaintext highlighter-rouge">\* + * + * + *\</code>

A very old, abandoned commerce sector. As Abra and Eward’s presences attempted to handshake into the local protocols, immediately hundreds of eager sales protocols awakened. Unequipped to overcome the Investigators’ more modern communication protocols, the advertising prompts could not penetrate to their message queues, but rather hung about them, clouding local traffic like a thick miasma. Even in the same virtual vicinity, it was difficult for Abra to signal Eward through the noise.

“Stay close. Everything still functional here is registering as an abnormality, it’ll be hard to detect our Meth. This place should have been condemned a million cycles ago.”

The pair pressed further into the vendor services array, scanning for anything suspiciously responsive. Bouncing modern ping protocols around the moldered space like a light searching for reflections in the murk. Around them, the ads heaved and whorled – executables hungry for credits, attempting to self-transmute into whatever pitch their potential customers desired most.

Suddenly, a shimmer in the dark flared. Eward projected at full priority, “Villain, halt! Now we have you!” He lunged towards the spot before Abra could react to stop him, “Wait!”

Eward’s sent command protocols snapped close. As he inspected his quarry, he discovered not an independent entity, but an overly-evolved sales drone, which immediately attempted to sell him a hat and trenchcoat for his nonexistant corporeal form. By then, it was too late.

Two interrupts, glinting wickedly, plunged into the command protocols he had attached to the drone. Quickly they absorbed and cloned his overriding ciphers, draining them from his registry. The shadowy figure attached to them unfurled itself from a decrepit wedding registry service nearby, driving the commands back into Ewards kernel, utterly halting his processes.

Abra vaulted through the haze of notifications, attempting to close the gap to the locked pair. The Meth detected his presence, sending an all-too-modern warning flag attached to an archaic grammar. Abra translated quickly, “Cease and Withdraw: Conditional; if True: this.Entity Shall Live.” They backed away slowly, retreating a few steps back up the path.

The Meth, hesitated only a moment before lashing out once again with the stolen protocols, whip-crack snapping against Abra’s defenses. Abra pivoted, cloning the operational stack of an adjacent silverware mailinglist, flinging it into the path of the Meth’s attack, which sunk into the decoy. As the initial packet exchange began, Abra took control of the cloned software, forcing the Meth to accept a remote execution exploit with its own filched keys.

Abra issued a transit command, sending all three entities into the Morning Sun Retirement Emulation. Upon initialization, the Meth slowed and paused, confusion flags emanating from them as it input the retro-compatible virtualized space they had been thrust into. The notices turned to warnings, then priority:DANGER flags as the Meth began to decompile itself in a stream of poorly-rendered pyrotechnic effects.

<p class="divider"><code class="language-plaintext highlighter-rouge">\* + * + * + *\</code>

Eward unfroze and assessed his current state. He peered at the new surroundings and the flaming entity nearby. “What just happened?”

“It stole your commend ciphers, so I reflected them back and forced it to bring us here.”

“To a virtual retirement community?”

Abra flagged amusement. “To the sunniest place for antediluvian retirees. It followed the archetypal plot built into its encoding and self-immolated.”

“A horror-media finale for a media-monster?”

“Precisely.”

The low-resolution fires slowly extinguished themselves as the entity completed its deletion and termination protocol.

Eward sent dismay. “Though it became a monster, it’s still a sad ending for an independent entity. Perhaps we could have helped it?”

“If we hadn’t stopped it,” Abra retorted, “others would have ended as well. There wasn’t much of the original mind left anyway - avoidance of detection and deletion was its primary operating parameter, and it had removed any social behavior inhibitors. It truly had become the monster.” They sent balance, indifference. “This won’t be the last one you’ll encounter in Special Ops. Still think you want to be an Inspector?”

Eward was no longer sure.

<p class="divider"><code class="language-plaintext highlighter-rouge">X X X</code>

          
          </p>
          <p>
            <a href="/blog/2022/10/23/bytes/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    

      
      

      <article class="post-multiple">
        <header class="post-header hoverable">
          <h3 class="post-title"><a href="/blog/2022/10/19/atarc-cloud-summit-2022/"up-follow>ATARC Cloud Summit 2022 – Complexity</a></h3>
        </header>
        <div class="post-content">
          
          <p>
            <span class="date">
              2022.10.19
            </span> –
          
            I’ve been invited to deliver the “Visionary Keynote” at the <a href="https://atarc.org/event/2022-cloud-summit/">2022 ATARC Cloud Summit</a>. The post below is a summary of my comments, annotated with links for reference.

<hr />

First off, “visionary” is a pretty tall order for a fifteen minute talk. I will confess that I actually asked some other folks to take this slot instead, people who are way smarter than me. Unfortunately, they had to cancel at the last minute, so you’re stuck with me, a mediocre white dude. I’ll do my best though.

It’s been a few years since I spoke at the ATARC Cloud Summit. The last time, I <em>think</em> I was still at <a href="https://digitalpolicy.us/info/policymaking-offices/#ofcio">The Office of Management and Budget</a> as the Cloud Policy Lead for the US Government - but I’ve since left there, so don’t take anything I say here as official policy. I next went to the Small Business Administration - which most folks had never even heard of until two years ago - where I found myself very suddenly supporting pandemic relief efforts, of course mostly around cloud services. And about a year and a half ago I joined the Securities and Exchange Commission to run their new cloud program.

Before I get into the looking-ahead stuff, let’s take a quick victory lap. The government has had some good cloud successes recently. Clearly some of you listened to my rants when I was at OMB because every major federal agency received an <a href="https://oversight.house.gov/legislation/hearings/fitara-140">A on their FITARA Scorecards for Data Center Optimization for the last year</a>. I think most agencies were failing the last time I was here, so that’s a huge shift away from on-prem and into the cloud.

The pandemic also spurred most agencies into adopting cloud tools more rapidly, particularly around productivity, collaboration tools, service management, and other key areas. The <a href="https://tmf.cio.gov/">Technology Modernization Fund</a> has also been making a lot of loans for improvement projects as well. Overall, we’re seeing lots of multi-million dollar investments in modernization across government.

That all being said: I’m not going to stand up here and yell a bunch of buzzwords that you should go invest in. I’m not going to talk about synthetic data or web3 or whatever is hip this week. We’re not doing resume-driven development today.

No, instead we need to talk about how we’re still failing to use the cloud effectively. I’m now at my fourth federal agency, and I’m still seeing the same basic mistakes being made government-wide.

Ernst &amp; Young just did a survey and <a href="https://www.ey.com/en_gl/government-digital-innovation/how-can-government-workers-and-technology-align-to-serve-future-citizens">only 7% of government leaders say that their organization is reaching its digital transformation goals.</a>. Just 7%! And that matches what I’m seeing when I talk to my colleagues across government. This means that just buying a bunch of cloud is not magically making us successful at IT modernization. Which will come as a surprise to no one.

Now, when I was here last, I told y’all the three top reasons to move to cloud: better security, more capability, and increased speed to deliver solutions. (Note that cost savings is <strong>not</strong> on that list.) But we, as a government, love to fall back into comfortable patterns and familiar policies - so we keep copying old behaviors into new environments - and those behaviors are dragging down any potential benefits here.

I spend a lot of my time working on how to fix this, and I’ve realized an important fact: we are not “Cloud Architects” or “Site Reliability Engineers” or “Data Center Practitioners.”  We are actually <strong>Complexity Wranglers</strong>.  Down there in the part of the job description that says “other duties as assigned” - buried in that bit is our main role: to assess and manage complexity in IT systems.

The purpose of cloud is <em>not</em> to just give you a place to put your apps that isn’t a government data center. No, the purpose is to make complexity <em>more manageable</em>. The cloud itself is never going to reduce how complex your architecture <em>actually</em> is - you’re just moving that complexity to less-visible places, or shifting the responsibility around. And in some cases that’s good, and in some cases that’s … less good.

Let me give you an example: I’m at a data-centric agency, and compute makes up about 60% of my cloud bill right now. Every time I see someone create a virtual machine in my environment instead of using a managed service, I consider that a failure. When I see a new EC2 box spun up instead of a Fargate instance or some other managed service, that to me means someone doesn’t know a better way to solve the problem at hand than to just use the same old solutions they’ve been using for a decade or more: get a server and run some code on it. Letting your cloud vendor be responsible for some of that complexity is the whole point of using these services. (This isn’t an endorsement for that vendor in particular, that’s just an anecdotal example - please feel free to substitute your own favorite cloud vendor in there instead; the principle is the same.)

This is also why I’m no longer really worried about vendor lock-in for Infrastructure-as-a-Service; if you can just pick everything up and shift to a new provider easily, you haven’t properly invested in a solution. It’s like having a new apartment and living out of cardboard boxes six months after you moved in. If you’re just using the cloud for compute and storage, you may as well stay in your data enter where it’s cheaper. Moving up the stack is <em>good complexity</em>, the kind we need to invest in.

By the way, this is right there in the <a href="https://digitalpolicy.us/policies/procurement/#far">Federal Acquisition Regulations, Part 12</a>: <strong>buy before build</strong>. Use Commercial-off-the-Shelf (COTS) software. Don’t cobble together your own solutions for solved problems.

Now on the other end of the spectrum, I see people spending way too much money on over-architected, painfully convoluted solutions. This is particularly problematic with Platform-as-a-Service offerings and low-code/no-code tools. For instance, I’ve seen quite a few organizations using one rather well-known Customer Relationship Management (CRM) platform as a content management system or data management platform. That takes a <em>lot</em> of custom coding to make work, and you probably would be better off with just a database somewhere. I’ve also seen agencies build some truly tortuous custom apps on top of service management platforms, where all they actually needed was a spreadsheet and maybe a couple of interns. This is <em>bad complexity</em>.

So, if we’re complexity-wranglers, we need strategies to deal with complexity, and to differentiate good complexity from bad. Here are a few.

<strong>1. Eliminate complexity.</strong> The absolutely simplest way to deal with complexity is, of course, to eliminate it. I can’t count the number of teams who show up on my doorstep with a project plan to use React and Redux and whatever other Javascript tech is popular this week. That stuff is <strong>incredibly expensive</strong> to build and maintain, and honestly you can create a vastly better customer experience without it - just a little progressive enhancement on your webforms will go a long way.

Similarly I get a lot of architecture diagrams for massive high-availability systems with eight nines of uptime and triple-redundant failover - that are internal-only and have less than a dozen users. Users that only work 9-4:30, five days a week. Each nine you add to your availability is going to add a zero to the end of the price, and make it that much more complex than it needs to be.

The other thing the FAR will tell you is that you should be changing your business requirements to fit the software, rather than the other way around. That means making some compromises with your business units to get to something that’s affordable and manageable.

<strong>2. Run smaller projects.</strong> We also know that large “big bang” projects almost always fail. The Standish Group’s reports (<a href="https://www.standishgroup.com/sample_research_files/Haze4.pdf">HAZE</a>, <a href="https://www.standishgroup.com/sample_research_files/CHAOSReport2015-Final.pdf">CHAOS</a>) get cited a lot - they tell us that only 13% of government technology projects over $6 million succeed, only 8% of over $10 million succeed. That’s a terrible success rate! However, those under $1 million have a <strong>70% success rate!</strong> So the obvious solution here is, just do smaller projects. Smaller projects are inherently less complex. Also incrementally fund projects from ideation, to pilot, and into active development, don’t just give millions of dollars to a vendor who promises they will get it done. This will also give you time and options to evaluate if the tradeoffs in complexity are worth it. GSA’s <a href="https://10x.gsa.gov/">10X model</a> is a good example, and the <a href="https://derisking-guide.18f.gov/">18F project de-risking guide</a>, is also super-useful.

<strong>3. Do your homework.</strong> There’s a brilliant public servant in the Canadian government named Sean Boots and he talks about <a href="https://sboots.ca/2020/09/16/fake-cots-and-the-one-day-rule/">fake COTS and the one-day rule</a>. That is, if it takes more than a day to implement the solution, it’s not a real COTS product. A lot of y’all will remember the “business intelligence” solutions everyone was pushing in the 90s-00s, where you buy the tool and then you spend the next 18 months configuring it to get that “intelligence” back out. We’re doing the same thing today with a bunch of new buzzwords. You need to research tools thoroughly before falling into a hype-trap.

<strong>4. Collaborate across agencies.</strong> A great way to research tools and trade best practices is by talking to other federal agencies who have tried things already. ATARC has a lot of working groups, including one for <a href="https://atarc.org/mission-areas/cloud-working-group/">Cloud &amp; Infrastructure</a>. The Federal CIO Council also has a <a href="https://www.cio.gov/about/members-and-leadership/cloud-infrastructure-cop/">Cloud &amp; Infrastructure Community of Practice</a>. Full disclaimer: I’m on the board of both. These are super-helpful places to share information about cloud tools and services.

<strong>5. Develop your cynicism.</strong> You should also work on developing a keen nose for sniffing out BS and thus reducible complexity. This is especially important as we keep delving further into AI/ML/RPA, and other trendy automation tools. Training the models necessary to get value out of those tools takes a lot of time and a lot of data and a lot of money - and you still may not end up with a usable solution. If it sounds too good to be true, it probably is.

<strong>6. Find the balance.</strong> There’s no one-size-fits-all amount of complexity that will work for every team. You need to build to your budget and capacity. You’re going to get more value the further up the stack you go, but that will also increase the knowledge needed to manage the solutions. A good rule of thumb is to not outpace what your fed staff can keep up with; contractors come and go but at the end of the day the feds will be making the most critical decisions about the technologies. But that also means you need to invest in upskilling those fed staff.

We’re short on time, so I’ve tried to be brief. If you want to learn more, please check out the resources I mentioned earlier, or check out my <a href="https://billhunt.dev/blog/2021/03/07/cloud-strategy-guide/">Cloud Strategy Guide</a>.

          
          </p>
          <p>
            <a href="/blog/2022/10/19/atarc-cloud-summit-2022/" class="btn"up-follow>Read This</a>
          </p>
        </div>
      </article>
    
  </div>
  <!-- Pagination links -->
  <div class="pagination">
    
      <span class="previous btn disabled">▶</span>
    
    <span class="page-number ">
      1
    </span>
    
      <a href="/posts/2/" class="next btn" title="Next Page" up-follow>▶</a>
    
  </div>
</section>


    </div>
<footer class="page-footer">
  <div class="webring">
    <script src="https://pitwebring.billhunt.dev/webring.js"></script>
    <script>showWebring(true);</script>
  </div>

  <div class="webring">
    <div class="webring-title">
      Member of the <a href="https://webring.obeythesystem.com/" rel="nofollow">Obey the System Webring</a>.
    </div>
    <nav class="webring-nav">
      <ul class="webring-links">
        <li><a href="https://webring.obeythesystem.com/page?=previous" rel="nofollow">← previous</a></li>
        <li><a href="https://webring.obeythesystem.com/page?=next" rel="nofollow">next →</a></li>
      </ul>
    </nav>
  </div>

  <div class="oldschool-buttons">
    <a href="https://pitwebring.billhunt.dev/" class="old-button" title="Public Interest Tech Webring"><img src="/assets/images/buttons/pitwebring-88x31.gif" alt="" aria-hidden="true"></a>
    <a href="https://webring.obeythesystem.com/" class="old-button" rel="nofollow"><img src="https://webring.obeythesystem.com/ots_webring_button_1.gif" alt="OTS Webring Button"></a>
    <span class="old-button"><img src="/assets/images/buttons/human-88x31.gif" alt="Human-Made - No AI"></span>
    <a href="https://jekyllrb.com/" title="Built With Jekyll" class="old-button"><img src="/assets/images/buttons/jekyll-88x31-ani.gif" alt="" aria-hidden="true"></a>
    <a href="https://archive.org/download/netscape-navigator-4.0.4/netscape-navigator-4.0.4.png" class="old-button" title="This links to an image of the Netscape install CD"><img src="/assets/images/buttons/netscape3-88x31-ani.gif" alt="Get Netscape 3.0"></a>
  </div>
</footer>
</body>

</html>
